import streamlit as st
import json
import requests
import re
import time
import graphviz

# å‡è®¾è¿™äº›æ¨¡å—éƒ½åœ¨æœ¬åœ°ç›®å½•
import config
from BGEReranker import BGEReranker
from BM25Retriever import BM25Retriever
from HQSmallDataLoader import HQSmallDataLoader
from denseInstructionRetriever import Qwen3Retriever
from denseRetriever import BGERetriever
from hybridRetrieveRerank import hybrid_retrieve_and_rerank
from prompts import DECOMPOSITION_PROMPT, RELEVANCE_CHECK_PROMPT, QUERY_REWRITE_PROMPT, GENERATE_ANSWER_PROMPT, \
    SELF_CHECK_PROMPT, SYNTHESIZE_ANSWERS_PROMPT


# ==========================================
# UI & æµç¨‹å›¾ è¾…åŠ©ç±»
# ==========================================

class PipelineVisualizer:
    def __init__(self):
        self.graph = graphviz.Digraph()
        self.graph.attr(rankdir='TB', size='10')
        self.current_step = "Start"
        self.logs = []

        # å®šä¹‰æµç¨‹å›¾çš„ç»“æ„ï¼ˆèŠ‚ç‚¹ï¼‰
        self.nodes = [
            "Start", "Init Retrievers", "Query Decomposition",
            "Retrieval & Rerank", "Relevance Check", "Query Rewrite",
            "Generate Answer", "Self-Check", "Synthesis", "End"
        ]

        # åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ä¸ºé»˜è®¤é¢œè‰²
        for node in self.nodes:
            self.graph.node(node, shape='box', style='rounded,filled', fillcolor='white', color='black')

        # å®šä¹‰è¾¹ï¼ˆè¿æ¥å…³ç³»ï¼‰
        self.graph.edge("Start", "Init Retrievers")
        self.graph.edge("Init Retrievers", "Query Decomposition")
        self.graph.edge("Query Decomposition", "Retrieval & Rerank")
        self.graph.edge("Retrieval & Rerank", "Relevance Check")
        self.graph.edge("Relevance Check", "Generate Answer", label="Yes")
        self.graph.edge("Relevance Check", "Query Rewrite", label="No")
        self.graph.edge("Query Rewrite", "Retrieval & Rerank")
        self.graph.edge("Generate Answer", "Self-Check")
        self.graph.edge("Self-Check", "Synthesis")
        self.graph.edge("Synthesis", "End")

    def update(self, step_name, log_message=None):
        """æ›´æ–°å½“å‰æ´»è·ƒèŠ‚ç‚¹å¹¶æ˜¾ç¤ºæ—¥å¿—"""
        self.current_step = step_name

        # é‡æ–°æ„å»ºå›¾ä»¥æ›´æ–°é¢œè‰²
        new_graph = graphviz.Digraph()
        new_graph.attr(rankdir='TB')

        for node in self.nodes:
            if node == step_name:
                # å½“å‰æ­¥éª¤é«˜äº®ä¸ºæ©™è‰²
                new_graph.node(node, shape='box', style='rounded,filled', fillcolor='#ff9f43', color='black',
                               fontcolor='white')
            else:
                new_graph.node(node, shape='box', style='rounded,filled', fillcolor='white', color='black')

        # é‡æ–°æ·»åŠ è¾¹
        new_graph.edge("Start", "Init Retrievers")
        new_graph.edge("Init Retrievers", "Query Decomposition")
        new_graph.edge("Query Decomposition", "Retrieval & Rerank")
        new_graph.edge("Retrieval & Rerank", "Relevance Check")
        new_graph.edge("Relevance Check", "Generate Answer", label="Relevant")
        new_graph.edge("Relevance Check", "Query Rewrite", label="Not Relevant")
        new_graph.edge("Query Rewrite", "Retrieval & Rerank")
        new_graph.edge("Generate Answer", "Self-Check")
        new_graph.edge("Self-Check", "Synthesis")
        new_graph.edge("Synthesis", "End")

        # åœ¨Streamlitä¸­æ¸²æŸ“å›¾è¡¨
        with st.session_state['graph_placeholder'].container():
            st.graphviz_chart(new_graph, use_container_width=True)

        # è®°å½•å¹¶æ˜¾ç¤ºæ—¥å¿—
        if log_message:
            st.session_state['logs'].append(f"**[{step_name}]**: {log_message}")
            with st.session_state['log_placeholder'].container():
                st.write(log_message)


# ==========================================
# æ ¸å¿ƒé€»è¾‘ (ç»è¿‡æ”¹é€ ä»¥é€‚é…UIæ›´æ–°)
# ==========================================

def clean_and_parse_json_response(response_text, step_name="", visualizer=None):
    # (ä¿æŒåŸé€»è¾‘ï¼Œç®€åŒ–printä¸ºpassï¼Œæˆ–ä½¿ç”¨visualizerè®°å½•)
    try:
        result = json.loads(response_text)
        return result
    except json.JSONDecodeError:
        pass

    cleaned_text = response_text.strip()
    cleaned_text = re.sub(r'^```json\s*', '', cleaned_text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r'```\s*$', '', cleaned_text)
    cleaned_text = re.sub(r'^json\s*', '', cleaned_text, flags=re.IGNORECASE)
    cleaned_text = cleaned_text.strip()

    try:
        result = json.loads(cleaned_text)
        return result
    except json.JSONDecodeError:
        try:
            start_idx = cleaned_text.find('{')
            end_idx = cleaned_text.rfind('}') + 1
            if start_idx != -1 and end_idx != 0:
                json_str = cleaned_text[start_idx:end_idx]
                return json.loads(json_str)
        except:
            pass

    return {"is_relevant": False, "reason": "JSON parsing failed", "suggested_rewrite": ""}


def call_llm(prompt, max_tokens=512, temperature=0.7, step_name="", expect_json=False, visualizer=None):
    if visualizer:
        # ç®€å•æ˜¾ç¤º Prompt çš„å‰ä¸€éƒ¨åˆ†ï¼Œé¿å…UIå¤ªä¹±
        visualizer.update(step_name, f"Sending Prompt to LLM...")

    headers = {
        "Authorization": f"Bearer {config.SF_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": config.SF_LLM_MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature
    }

    try:
        response = requests.post(config.SF_API_LLM_URL, headers=headers, json=data)
        if response.status_code == 200:
            result = response.json()["choices"][0]["message"]["content"].strip()
            if expect_json:
                return clean_and_parse_json_response(result, step_name)
            else:
                return result
        else:
            raise Exception(f"API Error: {response.text}")
    except Exception as e:
        if visualizer:
            visualizer.update(step_name, f"âŒ Error: {str(e)}")
        raise e


# ä½¿ç”¨ st.cache_resource ç¼“å­˜æ£€ç´¢å™¨ï¼Œé¿å…æ¯æ¬¡ç‚¹å‡»æŒ‰é’®éƒ½é‡æ–°åŠ è½½æ¨¡å‹
@st.cache_resource
def initialize_retrievers_cached():
    data_loader = HQSmallDataLoader(config.BASE_DATA_DIR)
    all_doc_ids, all_documents = data_loader.load_collection(config.COLLECTION_PATH)
    doc_id_to_text = dict(zip(all_doc_ids, all_documents))

    if config.SF_API_KEY:
        bge_reranker = BGEReranker(api_key=config.SF_API_KEY)
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever(api_key=config.SF_API_KEY)
        bge_retriever = BGERetriever(api_key=config.SF_API_KEY)
    else:
        # Fallback handling
        bge_reranker = BGEReranker()
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever()
        bge_retriever = BGERetriever()

    bm25_retriever.load_index(config.BM25_INDEX_PATH)
    bge_retriever.load_index(config.BGE_INDEX_DIR)
    qwen3_retriever.load_index(config.QWEN_INDEX_DIR)

    return bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text


def retrieve_documents(query, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text, retrieval_top_k=50,
                       rerank_top_k=10, visualizer=None):
    if visualizer:
        visualizer.update("Retrieval & Rerank", f"Retrieving for: '{query}'")

    results = hybrid_retrieve_and_rerank(
        query=query,
        first_retriever=bm25_retriever,
        second_retriever=bge_retriever,
        reranker=bge_reranker,
        doc_id_to_text_map=doc_id_to_text,
        retrieval_top_k=retrieval_top_k,
        rerank_top_k=rerank_top_k
    )
    doc_texts = [doc_id_to_text[doc_id] for doc_id, _ in results]
    doc_ids = [doc_id for doc_id, _ in results]
    return doc_texts, doc_ids


# ==========================================
# ä¸» Pipeline (ä¿®æ”¹ç‰ˆ)
# ==========================================

def rag_pipeline_web(query, visualizer):
    visualizer.update("Start", "Starting RAG Pipeline...")

    # åˆå§‹åŒ–
    visualizer.update("Init Retrievers", "Loading retrievers (cached)...")
    bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text = initialize_retrievers_cached()

    # 1. åˆ†è§£
    visualizer.update("Query Decomposition", f"Analyzing query: {query}")
    decomp_prompt = DECOMPOSITION_PROMPT.format(query=query)
    decomp_response = call_llm(decomp_prompt, step_name="Query Decomposition", expect_json=True, visualizer=visualizer)

    if isinstance(decomp_response, dict):
        needs_decomp = decomp_response.get("needs_decomposition", False)
        sub_queries = decomp_response.get("sub_queries", [])
    else:
        needs_decomp = False
        sub_queries = []

    queries = sub_queries if needs_decomp else [query]
    visualizer.update("Query Decomposition", f"Sub-queries: {queries}")

    sub_answers = []

    for i, q in enumerate(queries):
        current_query = q
        max_retries = 3
        is_relevant = False

        with st.expander(f"Processing Sub-query: {q}", expanded=True):
            for attempt in range(max_retries):
                st.write(f"ğŸ”„ **Attempt {attempt + 1}**")

                # æ£€ç´¢
                doc_texts, doc_ids = retrieve_documents(current_query, bm25_retriever, bge_retriever, bge_reranker,
                                                        doc_id_to_text, visualizer=visualizer)
                documents_str = "\n".join([f"Doc {i + 1}: {text}" for i, text in enumerate(doc_texts)])

                st.info(f"Retrieved {len(doc_texts)} documents.")

                # 2. ç›¸å…³æ€§æ£€æŸ¥
                visualizer.update("Relevance Check", f"Checking relevance for attempt {attempt + 1}")
                rel_prompt = RELEVANCE_CHECK_PROMPT.format(query=current_query, documents=documents_str)
                rel_response = call_llm(rel_prompt, step_name="Relevance Check", expect_json=True,
                                        visualizer=visualizer)

                if isinstance(rel_response, dict):
                    is_relevant = rel_response.get("is_relevant", False)
                    reason = rel_response.get("reason", "")
                    suggested_rewrite = rel_response.get("suggested_rewrite", "")
                else:
                    is_relevant = False
                    reason = "Parsing failed"
                    suggested_rewrite = ""

                if is_relevant:
                    st.success("âœ… Documents are relevant.")
                    break
                else:
                    st.warning(f"âš ï¸ Not relevant. Reason: {reason}")
                    visualizer.update("Query Rewrite", "Rewriting query...")
                    rewrite_prompt = QUERY_REWRITE_PROMPT.format(original_query=current_query, reason=reason,
                                                                 suggested_rewrite=suggested_rewrite)
                    current_query = call_llm(rewrite_prompt, step_name="Query Rewrite", visualizer=visualizer)
                    st.write(f"New Query: {current_query}")

            if not is_relevant:
                sub_answers.append("Insufficient information.")
                continue

            # 3. ç”Ÿæˆç­”æ¡ˆ
            visualizer.update("Generate Answer", "Generating answer based on context...")
            context = "\n\n".join(doc_texts)
            gen_prompt = GENERATE_ANSWER_PROMPT.format(query=current_query, context=context)
            gen_response = call_llm(gen_prompt, step_name="Generate Answer", visualizer=visualizer)

            if "\nEvidence: " in gen_response:
                answer, evidence = gen_response.split("\nEvidence: ", 1)
            else:
                answer = gen_response

            # 4. è‡ªæ£€
            visualizer.update("Self-Check", "Verifying answer accuracy...")
            self_check_prompt = SELF_CHECK_PROMPT.format(answer=answer, documents=documents_str)
            self_check_response = call_llm(self_check_prompt, step_name="Self-Check", expect_json=True,
                                           visualizer=visualizer)

            if isinstance(self_check_response, dict):
                is_valid = self_check_response.get("is_valid", False)
                revised_answer = self_check_response.get("revised_answer", "")
            else:
                is_valid = True  # Default to trust if check fails
                revised_answer = ""

            final_sub_answer = revised_answer if (not is_valid and revised_answer) else answer
            sub_answers.append(final_sub_answer)
            st.markdown(f"**Sub-Answer:** {final_sub_answer}")

    # 5. åˆæˆ
    visualizer.update("Synthesis", "Synthesizing final answer...")
    if needs_decomp and len(sub_answers) > 1:
        sub_answers_str = "\n".join([f"Sub-answer {i + 1}: {answer}" for i, answer in enumerate(sub_answers)])
        synth_prompt = SYNTHESIZE_ANSWERS_PROMPT.format(original_query=query, sub_answers=sub_answers_str)
        final_answer = call_llm(synth_prompt, step_name="Synthesis", visualizer=visualizer)
    else:
        final_answer = sub_answers[0] if sub_answers else "No answer generated"

    visualizer.update("End", "Process Completed.")
    return final_answer


# ==========================================
# Streamlit é¡µé¢å¸ƒå±€
# ==========================================

st.set_page_config(page_title="RAG Workflow Visualizer", layout="wide")

st.title("ğŸ¤– Interactive RAG Pipeline")
st.markdown("This tool visualizes the retrieval-augmented generation process step-by-step.")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("Settings")
    st.write("Current Model:", config.SF_LLM_MODEL_NAME)
    if st.button("Clear History"):
        st.session_state['logs'] = []
        st.rerun()

# åˆå§‹åŒ– Session State
if 'logs' not in st.session_state:
    st.session_state['logs'] = []

# å¸ƒå±€ï¼šå·¦ä¾§æ˜¯æµç¨‹å›¾ï¼Œå³ä¾§æ˜¯äº¤äº’å’Œè¯¦ç»†æ—¥å¿—
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ğŸ“Š Live Workflow")
    # å ä½ç¬¦ç”¨äºåŠ¨æ€æ›´æ–°æµç¨‹å›¾
    if 'graph_placeholder' not in st.session_state:
        st.session_state['graph_placeholder'] = st.empty()

    # åˆå§‹åŒ–æ˜¾ç¤ºä¸€ä¸ªé™æ€å›¾
    viz = PipelineVisualizer()
    viz.update("Start")  # Render initial state

with col2:
    st.subheader("ğŸ’¬ Query & Process")

    user_query = st.text_input("Enter your question:",
                               "Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?")

    start_btn = st.button("ğŸš€ Start Search", type="primary")

    # ç”¨äºæ˜¾ç¤ºå®æ—¶æ—¥å¿—çš„å ä½ç¬¦
    st.session_state['log_placeholder'] = st.empty()

    result_container = st.container()

    if start_btn and user_query:
        st.session_state['logs'] = []  # Clear old logs

        with st.spinner("Running RAG Pipeline..."):
            try:
                # è¿è¡Œ Pipeline
                final_answer = rag_pipeline_web(user_query, viz)

                # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                with result_container:
                    st.success("ğŸ‰ Final Answer Generated!")
                    st.markdown(f"### Answer:\n{final_answer}")

            except Exception as e:
                st.error(f"An error occurred: {e}")
                # æ‰“å°å †æ ˆä»¥ä¾¿è°ƒè¯•
                import traceback

                st.code(traceback.format_exc())

# åœ¨é¡µé¢åº•éƒ¨æ˜¾ç¤ºå†å²è¯¦ç»†æ—¥å¿—
with st.expander("ğŸ“œ View Detailed Execution Logs"):
    for log in st.session_state['logs']:
        st.write(log)