import json
import requests
import re
import config  # å‡è®¾ config.py å­˜åœ¨ï¼ŒåŒ…å« BASE_DATA_DIR, SF_API_KEY ç­‰é…ç½®
from BGEReranker import BGEReranker  # BGE é‡æ’åºå™¨æ¨¡å—
from BM25Retriever import BM25Retriever  # BM25 æ£€ç´¢å™¨æ¨¡å—
from HQSmallDataLoader import HQSmallDataLoader  # HotpotQA å°å‹æ•°æ®é›†åŠ è½½å™¨
from denseInstructionRetriever import Qwen3Retriever  # Qwen3 å¯†é›†æ£€ç´¢å™¨
from denseRetriever import BGERetriever  # BGE å¯†é›†æ£€ç´¢å™¨
from hybridRetrieveRerank import hybrid_retrieve_and_rerank  # æ··åˆæ£€ç´¢å’Œé‡æ’åºå‡½æ•°
from prompts import DECOMPOSITION_PROMPT, RELEVANCE_AND_REWRITE_PROMPT, GENERATE_ANSWER_PROMPT, \
    SELF_CHECK_PROMPT, SYNTHESIZE_ANSWERS_PROMPT  # å¯¼å…¥æ–°çš„æç¤ºæ¨¡æ¿


# å‡½æ•°ï¼šæ¸…ç†å’Œè§£æ JSON å“åº”
def clean_and_parse_json_response(response_text, step_name=""):
    """
    æ¸…ç†å“åº”æ–‡æœ¬ä¸­çš„ JSON æ ‡è®°å¹¶è§£æä¸ºå­—å…¸ã€‚
    å‚æ•°:
    response_text (str): åŒ…å« JSON çš„å“åº”æ–‡æœ¬
    step_name (str): æ­¥éª¤åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•
    è¿”å›:
    dict: è§£æåçš„ JSON å­—å…¸
    """
    print(f"ğŸ§¹ Cleaning JSON response for {step_name}...")
    print(f"ğŸ“¥ Raw response: {response_text}")

    # å°è¯•ç›´æ¥è§£æ
    try:
        result = json.loads(response_text)
        print(f"   âœ… Direct JSON parsing successful")
        return result
    except json.JSONDecodeError:
        print(f"   âš ï¸ Direct parsing failed, attempting to extract JSON from markdown code blocks")
        pass

    # æ¸…ç†å¸¸è§çš„ JSON æ ‡è®°
    cleaned_text = response_text.strip()

    # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
    cleaned_text = re.sub(r'^```json\s*', '', cleaned_text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r'```\s*$', '', cleaned_text)

    # ç§»é™¤å…¶ä»–å¯èƒ½çš„ä»£ç å—æ ‡è®°
    cleaned_text = re.sub(r'^```\s*', '', cleaned_text)
    cleaned_text = re.sub(r'```\s*$', '', cleaned_text)

    # ç§»é™¤å¼€å¤´çš„ "json" å­—æ ·
    cleaned_text = re.sub(r'^json\s*', '', cleaned_text, flags=re.IGNORECASE)

    cleaned_text = cleaned_text.strip()
    print(f"   ğŸ”§ Cleaned text: {cleaned_text}")

    # å°è¯•è§£ææ¸…ç†åçš„æ–‡æœ¬
    try:
        result = json.loads(cleaned_text)
        print(f"   âœ… Cleaned JSON parsing successful")
        return result
    except json.JSONDecodeError as e:
        print(f"   âŒ Failed to parse JSON after cleaning: {e}")
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾çš„æå–æ–¹æ³•
        try:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
            start_idx = cleaned_text.find('{')
            end_idx = cleaned_text.rfind('}') + 1
            if start_idx != -1 and end_idx != 0:
                json_str = cleaned_text[start_idx:end_idx]
                result = json.loads(json_str)
                print(f"   âœ… Extracted JSON parsing successful")
                return result
        except Exception as e2:
            print(f"   âŒ All JSON parsing attempts failed: {e2}")

    # å¦‚æœæ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
    default_result = {
        "is_relevant": False,
        "reason": "JSON parsing failed",
        "improved_query": ""
    }
    print(f"   âš ï¸ Returning default result due to parsing failure")
    return default_result


# å‡½æ•°ï¼šè°ƒç”¨ SiliconFlow API æ¥ç”Ÿæˆ LLM å“åº”
def call_llm(prompt, max_tokens=512, temperature=0.7, step_name="", expect_json=False):
    """
    è°ƒç”¨ SiliconFlow API æ¥å¤„ç†ç»™å®šçš„æç¤ºã€‚
    å‚æ•°:
    prompt (str): è¾“å…¥çš„æç¤ºæ–‡æœ¬ã€‚
    max_tokens (int): æœ€å¤§ç”Ÿæˆçš„ token æ•°é‡ï¼Œé»˜è®¤ 512ã€‚
    temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.7ã€‚
    step_name (str): å½“å‰æ­¥éª¤åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
    expect_json (bool): æ˜¯å¦æœŸæœ›è¿”å› JSON æ ¼å¼ï¼Œé»˜è®¤ Falseã€‚
    è¿”å›:
    str: API è¿”å›çš„å“åº”å†…å®¹ã€‚
    å¼‚å¸¸:
    å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ã€‚
    """
    print(f"{'-' * 80}")
    print(f"ğŸ¤– LLM CALL - {step_name}")
    print(f"ğŸ“¤ PROMPT SENT:\n{prompt}")
    print(f"{'-' * 80}")

    headers = {
        "Authorization": f"Bearer {config.SF_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": config.SF_LLM_MODEL_NAME,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(config.SF_API_LLM_URL, headers=headers, json=data)
    if response.status_code == 200:
        result = response.json()["choices"][0]["message"]["content"].strip()
        print(f"ğŸ“¥ RESPONSE RECEIVED:\n{result}")
        print(f"{'=' * 80}")

        # å¦‚æœæœŸæœ› JSON æ ¼å¼ï¼Œè¿›è¡Œæ¸…ç†å’Œè§£æ
        if expect_json:
            return clean_and_parse_json_response(result, step_name)
        else:
            return result
    else:
        error_msg = f"API call failed: {response.text}"
        print(f"âŒ ERROR: {error_msg}")
        print(f"{'=' * 80}")
        raise Exception(error_msg)


# å‡½æ•°ï¼šåˆå§‹åŒ–æ£€ç´¢å™¨
def initialize_retrievers():
    """
    åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’Œå„ç§æ£€ç´¢å™¨ã€‚
    è¿”å›:
    tuple: åŒ…å« BM25 æ£€ç´¢å™¨ã€BGE æ£€ç´¢å™¨ã€Qwen3 æ£€ç´¢å™¨ã€BGE é‡æ’åºå™¨ å’Œ æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    """
    print("ğŸ”„ Initializing retrievers...")
    data_loader = HQSmallDataLoader(config.BASE_DATA_DIR)
    all_doc_ids, all_documents = data_loader.load_collection(config.COLLECTION_PATH)
    doc_id_to_text = dict(zip(all_doc_ids, all_documents))

    if config.SF_API_KEY:
        bge_reranker = BGEReranker(api_key=config.SF_API_KEY)
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever(api_key=config.SF_API_KEY)
        bge_retriever = BGERetriever(api_key=config.SF_API_KEY)
    else:
        bge_reranker = BGEReranker()
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever()
        bge_retriever = BGERetriever()

    bm25_retriever.load_index(config.BM25_INDEX_PATH)
    bge_retriever.load_index(config.BGE_INDEX_DIR)
    qwen3_retriever.load_index(config.QWEN_INDEX_DIR)
    print("âœ… Retrievers initialized successfully")
    return bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text


# å‡½æ•°ï¼šæ£€ç´¢å’Œæ ¼å¼åŒ–æ–‡æ¡£ï¼ˆåªå–50åˆ†ä»¥ä¸Šçš„ç»“æœï¼‰
def retrieve_documents(query, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text, retrieval_top_k=50,
                       rerank_top_k=10, min_score=50):
    """
    ä½¿ç”¨æ··åˆæ£€ç´¢å’Œé‡æ’åºæ¥æ£€ç´¢æ–‡æ¡£ï¼Œåªè¿”å›åˆ†æ•°é«˜äºé˜ˆå€¼çš„æ–‡æ¡£ã€‚
    å‚æ•°:
    query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
    bm25_retriever: BM25 æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_retriever: BGE æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_reranker: BGE é‡æ’åºå™¨å®ä¾‹ã€‚
    doc_id_to_text (dict): æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    retrieval_top_k (int): åˆå§‹æ£€ç´¢çš„ top k å€¼ï¼Œé»˜è®¤ 50ã€‚
    rerank_top_k (int): é‡æ’åºåçš„ top k å€¼ï¼Œé»˜è®¤ 10ã€‚
    min_score (int): æœ€å°åˆ†æ•°é˜ˆå€¼ï¼Œé»˜è®¤ 50ã€‚
    è¿”å›:
    tuple: åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨å’Œæ–‡æ¡£ ID åˆ—è¡¨ã€‚
    """
    print(f"ğŸ” RETRIEVING DOCUMENTS FOR QUERY: '{query}'")
    print(f"Retrieval top_k: {retrieval_top_k}, Rerank top_k: {rerank_top_k}, Min score: {min_score}")

    results = hybrid_retrieve_and_rerank(
        query=query,
        first_retriever=bm25_retriever,
        second_retriever=bge_retriever,
        reranker=bge_reranker,
        doc_id_to_text_map=doc_id_to_text,
        retrieval_top_k=retrieval_top_k,
        rerank_top_k=rerank_top_k
    )

    # è¿‡æ»¤åˆ†æ•°é«˜äºé˜ˆå€¼çš„æ–‡æ¡£
    filtered_results = [(doc_id, score) for doc_id, score in results if score >= min_score]

    if not filtered_results:
        print(f"âš ï¸ No documents found with score >= {min_score}, using top document regardless of score")
        filtered_results = [results[0]] if results else []

    doc_texts = [doc_id_to_text[doc_id] for doc_id, _ in filtered_results]
    doc_ids = [doc_id for doc_id, _ in filtered_results]

    print(f"âœ… Retrieved {len(doc_texts)} documents (score >= {min_score})")
    for i, (doc_id, (_, score)) in enumerate(zip(doc_ids, filtered_results)):
        print(f"   Document {i + 1} (ID: {doc_id}, Score: {score:.2f}): {doc_texts[i][:100]}...")

    return doc_texts, doc_ids


# å‡½æ•°ï¼šå¤„ç†å•ä¸ªæŸ¥è¯¢
def process_single_query(query, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text):
    """
    å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼ŒåŒ…æ‹¬æ£€ç´¢ã€ç›¸å…³æ€§åˆ¤æ–­ã€é‡å†™å’Œç­”æ¡ˆç”Ÿæˆã€‚
    å‚æ•°:
    query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
    bm25_retriever: BM25 æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_retriever: BGE æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_reranker: BGE é‡æ’åºå™¨å®ä¾‹ã€‚
    doc_id_to_text (dict): æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    è¿”å›:
    str: ç”Ÿæˆçš„ç­”æ¡ˆã€‚
    """
    current_query = query
    max_retries = 3

    for attempt in range(max_retries):
        print(f"ğŸ” ATTEMPT {attempt + 1}/{max_retries}")
        print(f"   Current query: '{current_query}'")

        # æ£€ç´¢æ–‡æ¡£ï¼ˆåªå–50åˆ†ä»¥ä¸Šçš„ç»“æœï¼‰
        doc_texts, doc_ids = retrieve_documents(current_query, bm25_retriever, bge_retriever, bge_reranker,
                                                doc_id_to_text)
        documents_str = "\n".join([f"Doc {i + 1}: {text}" for i, text in enumerate(doc_texts)])

        # æ­¥éª¤ 2: ç›¸å…³æ€§åˆ¤æ–­ + æŸ¥è¯¢é‡å†™ï¼ˆåˆå¹¶æ­¥éª¤ï¼‰
        print("=" * 100)
        print(f"ğŸ“Š STEP 2: RELEVANCE AND REWRITE")
        rel_rewrite_prompt = RELEVANCE_AND_REWRITE_PROMPT.format(
            query=current_query,
            documents=documents_str
        )
        rel_rewrite_response = call_llm(
            rel_rewrite_prompt,
            step_name=f"RELEVANCE AND REWRITE (Attempt {attempt + 1})",
            expect_json=True
        )

        # å¤„ç†å“åº”
        if isinstance(rel_rewrite_response, dict):
            is_relevant = rel_rewrite_response.get("is_relevant", False)
            reason = rel_rewrite_response.get("reason", "")
            improved_query = rel_rewrite_response.get("improved_query", "")
            print(f"   âœ… Relevance and rewrite result: is_relevant={is_relevant}, reason={reason}")
            if improved_query:
                print(f"   ğŸ’¡ Improved query: {improved_query}")
        else:
            print(f"   âŒ Unexpected response type for relevance and rewrite: {type(rel_rewrite_response)}")
            is_relevant = False
            reason = "Unexpected response type"
            improved_query = ""

        if is_relevant:
            print(f"   âœ… Documents are relevant, proceeding to answer generation")
            break
        else:
            print(f"   âš ï¸ Documents not relevant, using improved query for next attempt")
            if improved_query and improved_query.strip():
                current_query = improved_query.strip()
                print(f"   ğŸ”„ Using improved query: '{current_query}'")
            else:
                print(f"   âš ï¸ No improved query provided, using original query")
                current_query = query

    if not is_relevant:
        print(f"   âŒ Failed to find relevant documents after {max_retries} attempts")
        return "æ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•ç¡®å®š"

    # æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ
    print("=" * 100)
    print(f"ğŸ“ STEP 3: GENERATE ANSWER")
    context = "\n\n".join(doc_texts)
    gen_prompt = GENERATE_ANSWER_PROMPT.format(query=current_query, context=context)
    answer = call_llm(gen_prompt, step_name="GENERATE ANSWER")
    print(f"âœ… Answer generated: {answer}")

    # æ­¥éª¤ 4: ç­”æ¡ˆè‡ªæ£€
    print("=" * 100)
    print(f"âœ… STEP 4: SELF-CHECK")
    self_check_prompt = SELF_CHECK_PROMPT.format(
        query=current_query,
        answer=answer,
        documents=documents_str
    )
    self_check_response = call_llm(self_check_prompt, step_name="SELF-CHECK", expect_json=True)

    # å¤„ç†è‡ªæ£€å“åº”
    if isinstance(self_check_response, dict):
        is_valid = self_check_response.get("is_valid", False)
        issues = self_check_response.get("issues", "")
        revised_answer = self_check_response.get("revised_answer", "")
        print(f"ğŸ” Self-check result: is_valid={is_valid}, issues={issues}")
        if revised_answer and revised_answer.strip():
            print(f"ğŸ“ Using revised answer: {revised_answer}")
            return revised_answer.strip()
    else:
        print(f"âŒ Unexpected response type for self-check: {type(self_check_response)}")

    return answer


# ç®¡é“å‡½æ•°ï¼šRAG ç®¡é“å®ç°
def rag_pipeline(query):
    """
    RAG (Retrieval-Augmented Generation) ç®¡é“çš„ä¸»å‡½æ•°ã€‚
    å¤„ç†æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åˆ†è§£ã€æ£€ç´¢ã€ç›¸å…³æ€§åˆ¤æ–­ã€é‡å†™ã€ç”Ÿæˆç­”æ¡ˆã€è‡ªæ£€å’Œåˆæˆã€‚
    å‚æ•°:
    query (str): è¾“å…¥æŸ¥è¯¢ã€‚
    è¿”å›:
    str: æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆã€‚
    """
    print(f"ğŸ¯ STARTING RAG PIPELINE FOR QUERY: '{query}'")

    bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text = initialize_retrievers()

    # æ­¥éª¤ 1: æŸ¥è¯¢åˆ†è§£
    print("=" * 100)
    print(f"ğŸ“ STEP 1: QUERY DECOMPOSITION")
    print(f"Original query: '{query}'")

    decomp_prompt = DECOMPOSITION_PROMPT.format(query=query)
    decomp_response = call_llm(decomp_prompt, step_name="QUERY DECOMPOSITION", expect_json=True)

    # å¤„ç†åˆ†è§£å“åº”
    if isinstance(decomp_response, dict):
        needs_decomp = decomp_response.get("needs_decomposition", False)
        sub_queries = decomp_response.get("sub_queries", [])
        print(f"âœ… Decomposition result: needs_decomposition={needs_decomp}, sub_queries={sub_queries}")
    else:
        print(f"âŒ Unexpected response type for decomposition: {type(decomp_response)}")
        needs_decomp = False
        sub_queries = []

    queries = sub_queries if needs_decomp and sub_queries else [query]
    print(f"ğŸ“‹ Queries to process: {queries}")

    sub_answers_with_queries = []

    for i, q in enumerate(queries):
        print("=" * 100)
        print(f"ğŸ”„ PROCESSING SUB-QUERY {i + 1}/{len(queries)}: '{q}'")

        answer = process_single_query(q, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text)
        sub_answers_with_queries.append(f"Sub-query: {q}\nAnswer: {answer}")
        print(f"âœ… Sub-answer {i + 1} completed: {answer[:100]}...")

    # æ­¥éª¤ 5: å¤šå­ç­”æ¡ˆåˆæˆï¼ˆå¦‚æœéœ€è¦ï¼‰
    print("=" * 100)
    print(f"ğŸ¯ FINAL STEP: SYNTHESIZE ANSWERS")
    if needs_decomp and len(sub_answers_with_queries) > 1:
        print(f"ğŸ“¦ Synthesizing {len(sub_answers_with_queries)} sub-answers into final answer")
        sub_answers_str = "\n\n".join(sub_answers_with_queries)
        synth_prompt = SYNTHESIZE_ANSWERS_PROMPT.format(
            original_query=query,
            sub_answers_with_queries=sub_answers_str
        )
        final_answer = call_llm(synth_prompt, step_name="SYNTHESIZE ANSWERS")
        print(f"âœ… Final synthesized answer ready")
    else:
        final_answer = sub_answers_with_queries[0].split("Answer: ")[
            1] if sub_answers_with_queries else "No answer generated"
        print(f"âœ… Using single answer as final answer")

    return final_answer


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    sample_query = "Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?"
    print("ğŸš€ STARTING RAG PIPELINE DEMO")
    print("=" * 100)
    answer = rag_pipeline(sample_query)
    print(f"ğŸ‰ FINAL RESULT")
    print("=" * 100)
    print(f"ğŸ“ Original Query: {sample_query}")
    print(f"ğŸ’¡ Final Answer: {answer}")
    print("=" * 100)