import json
import requests
import re
import config  # å‡è®¾ config.py å­˜åœ¨ï¼ŒåŒ…å« BASE_DATA_DIR, SF_API_KEY ç­‰é…ç½®
from BGEReranker import BGEReranker  # BGE é‡æ’åºå™¨æ¨¡å—
from BM25Retriever import BM25Retriever  # BM25 æ£€ç´¢å™¨æ¨¡å—
from HQSmallDataLoader import HQSmallDataLoader  # HotpotQA å°å‹æ•°æ®é›†åŠ è½½å™¨
from denseInstructionRetriever import Qwen3Retriever  # Qwen3 å¯†é›†æ£€ç´¢å™¨
from denseRetriever import BGERetriever  # BGE å¯†é›†æ£€ç´¢å™¨
from hybridRetrieveRerank import hybrid_retrieve_and_rerank  # æ··åˆæ£€ç´¢å’Œé‡æ’åºå‡½æ•°
from prompts import DECOMPOSITION_PROMPT, RELEVANCE_CHECK_PROMPT, QUERY_REWRITE_PROMPT, GENERATE_ANSWER_PROMPT, \
    SELF_CHECK_PROMPT, SYNTHESIZE_ANSWERS_PROMPT  # å¯¼å…¥é¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿


# å‡½æ•°ï¼šæ¸…ç†å’Œè§£æ JSON å“åº”
def clean_and_parse_json_response(response_text, step_name=""):
    """
    æ¸…ç†å“åº”æ–‡æœ¬ä¸­çš„ JSON æ ‡è®°å¹¶è§£æä¸ºå­—å…¸ã€‚
    å‚æ•°:
    response_text (str): åŒ…å« JSON çš„å“åº”æ–‡æœ¬
    step_name (str): æ­¥éª¤åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•
    è¿”å›:
    dict: è§£æåçš„ JSON å­—å…¸
    """
    print(f"ğŸ§¹ Cleaning JSON response for {step_name}...")
    print(f"ğŸ“¥ Raw response: {response_text}")

    # å°è¯•ç›´æ¥è§£æ
    try:
        result = json.loads(response_text)
        print(f"   âœ… Direct JSON parsing successful")
        return result
    except json.JSONDecodeError:
        print(f"   âš ï¸ Direct parsing failed, attempting to extract JSON from markdown code blocks")
        pass

    # æ¸…ç†å¸¸è§çš„ JSON æ ‡è®°
    cleaned_text = response_text.strip()

    # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
    cleaned_text = re.sub(r'^```json\s*', '', cleaned_text, flags=re.IGNORECASE)
    cleaned_text = re.sub(r'```\s*$', '', cleaned_text)

    # ç§»é™¤å…¶ä»–å¯èƒ½çš„ä»£ç å—æ ‡è®°
    cleaned_text = re.sub(r'^```\s*', '', cleaned_text)
    cleaned_text = re.sub(r'```\s*$', '', cleaned_text)

    # ç§»é™¤å¼€å¤´çš„ "json" å­—æ ·
    cleaned_text = re.sub(r'^json\s*', '', cleaned_text, flags=re.IGNORECASE)

    cleaned_text = cleaned_text.strip()
    print(f"   ğŸ”§ Cleaned text: {cleaned_text}")

    # å°è¯•è§£ææ¸…ç†åçš„æ–‡æœ¬
    try:
        result = json.loads(cleaned_text)
        print(f"   âœ… Cleaned JSON parsing successful")
        return result
    except json.JSONDecodeError as e:
        print(f"   âŒ Failed to parse JSON after cleaning: {e}")
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾çš„æå–æ–¹æ³•
        try:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
            start_idx = cleaned_text.find('{')
            end_idx = cleaned_text.rfind('}') + 1
            if start_idx != -1 and end_idx != 0:
                json_str = cleaned_text[start_idx:end_idx]
                result = json.loads(json_str)
                print(f"   âœ… Extracted JSON parsing successful")
                return result
        except Exception as e2:
            print(f"   âŒ All JSON parsing attempts failed: {e2}")

    # å¦‚æœæ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
    default_result = {
        "is_relevant": False,
        "reason": "JSON parsing failed",
        "suggested_rewrite": ""
    }
    print(f"   âš ï¸ Returning default result due to parsing failure")
    return default_result


# å‡½æ•°ï¼šè°ƒç”¨ SiliconFlow API æ¥ç”Ÿæˆ LLM å“åº”
def call_llm(prompt, max_tokens=512, temperature=0.7, step_name="", expect_json=False):
    """
    è°ƒç”¨ SiliconFlow API æ¥å¤„ç†ç»™å®šçš„æç¤ºã€‚
    å‚æ•°:
    prompt (str): è¾“å…¥çš„æç¤ºæ–‡æœ¬ã€‚
    max_tokens (int): æœ€å¤§ç”Ÿæˆçš„ token æ•°é‡ï¼Œé»˜è®¤ 512ã€‚
    temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.7ã€‚
    step_name (str): å½“å‰æ­¥éª¤åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
    expect_json (bool): æ˜¯å¦æœŸæœ›è¿”å› JSON æ ¼å¼ï¼Œé»˜è®¤ Falseã€‚
    è¿”å›:
    str: API è¿”å›çš„å“åº”å†…å®¹ã€‚
    å¼‚å¸¸:
    å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ã€‚
    """
    print(f"{'-' * 80}")
    print(f"ğŸ¤– LLM CALL - {step_name}")
    print(f"ğŸ“¤ PROMPT SENT:\n{prompt}")
    print(f"{'-' * 80}")

    headers = {
        "Authorization": f"Bearer {config.SF_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": config.SF_LLM_MODEL_NAME,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(config.SF_API_LLM_URL, headers=headers, json=data)
    if response.status_code == 200:
        result = response.json()["choices"][0]["message"]["content"].strip()
        print(f"ğŸ“¥ RESPONSE RECEIVED:\n{result}")
        print(f"{'=' * 80}")

        # å¦‚æœæœŸæœ› JSON æ ¼å¼ï¼Œè¿›è¡Œæ¸…ç†å’Œè§£æ
        if expect_json:
            return clean_and_parse_json_response(result, step_name)
        else:
            return result
    else:
        error_msg = f"API call failed: {response.text}"
        print(f"âŒ ERROR: {error_msg}")
        print(f"{'=' * 80}")
        raise Exception(error_msg)


# å‡½æ•°ï¼šåˆå§‹åŒ–æ£€ç´¢å™¨
def initialize_retrievers():
    """
    åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’Œå„ç§æ£€ç´¢å™¨ã€‚
    è¿”å›:
    tuple: åŒ…å« BM25 æ£€ç´¢å™¨ã€BGE æ£€ç´¢å™¨ã€Qwen3 æ£€ç´¢å™¨ã€BGE é‡æ’åºå™¨ å’Œ æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    """
    print("ğŸ”„ Initializing retrievers...")
    data_loader = HQSmallDataLoader(config.BASE_DATA_DIR)
    all_doc_ids, all_documents = data_loader.load_collection(config.COLLECTION_PATH)
    doc_id_to_text = dict(zip(all_doc_ids, all_documents))

    if config.SF_API_KEY:
        bge_reranker = BGEReranker(api_key=config.SF_API_KEY)
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever(api_key=config.SF_API_KEY)
        bge_retriever = BGERetriever(api_key=config.SF_API_KEY)
    else:
        bge_reranker = BGEReranker()
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever()
        bge_retriever = BGERetriever()

    bm25_retriever.load_index(config.BM25_INDEX_PATH)
    bge_retriever.load_index(config.BGE_INDEX_DIR)
    qwen3_retriever.load_index(config.QWEN_INDEX_DIR)
    print("âœ… Retrievers initialized successfully")
    return bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text


# å‡½æ•°ï¼šæ£€ç´¢å’Œæ ¼å¼åŒ–æ–‡æ¡£
def retrieve_documents(query, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text, retrieval_top_k=50,
                       rerank_top_k=10):
    """
    ä½¿ç”¨æ··åˆæ£€ç´¢å’Œé‡æ’åºæ¥æ£€ç´¢æ–‡æ¡£ã€‚
    å‚æ•°:
    query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
    bm25_retriever: BM25 æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_retriever: BGE æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_reranker: BGE é‡æ’åºå™¨å®ä¾‹ã€‚
    doc_id_to_text (dict): æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    retrieval_top_k (int): åˆå§‹æ£€ç´¢çš„ top k å€¼ï¼Œé»˜è®¤ 50ã€‚
    rerank_top_k (int): é‡æ’åºåçš„ top k å€¼ï¼Œé»˜è®¤ 10ã€‚
    è¿”å›:
    tuple: åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨å’Œæ–‡æ¡£ ID åˆ—è¡¨ã€‚
    """
    print(f"ğŸ” RETRIEVING DOCUMENTS FOR QUERY: '{query}'")
    print(f"Retrieval top_k: {retrieval_top_k}, Rerank top_k: {rerank_top_k}")

    results = hybrid_retrieve_and_rerank(
        query=query,
        first_retriever=bm25_retriever,
        second_retriever=bge_retriever,
        reranker=bge_reranker,
        doc_id_to_text_map=doc_id_to_text,
        retrieval_top_k=retrieval_top_k,
        rerank_top_k=rerank_top_k
    )
    doc_texts = [doc_id_to_text[doc_id] for doc_id, _ in results]
    doc_ids = [doc_id for doc_id, _ in results]

    print(f"âœ… Retrieved {len(doc_texts)} documents")
    for i, (doc_id, text) in enumerate(zip(doc_ids, doc_texts)):
        print(f"   Document {i + 1} (ID: {doc_id}): {text[:100]}...")

    return doc_texts, doc_ids


# ç®¡é“å‡½æ•°ï¼šRAG ç®¡é“å®ç°
def rag_pipeline(query):
    """
    RAG (Retrieval-Augmented Generation) ç®¡é“çš„ä¸»å‡½æ•°ã€‚
    å¤„ç†æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åˆ†è§£ã€æ£€ç´¢ã€ç›¸å…³æ€§æ£€æŸ¥ã€é‡å†™ã€ç”Ÿæˆç­”æ¡ˆã€è‡ªæ£€å’Œåˆæˆã€‚
    å‚æ•°:
    query (str): è¾“å…¥æŸ¥è¯¢ã€‚
    è¿”å›:
    str: æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆã€‚
    """
    print(f"ğŸ¯ STARTING RAG PIPELINE FOR QUERY: '{query}'")

    bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text = initialize_retrievers()

    # æ­¥éª¤ 1: æŸ¥è¯¢åˆ†è§£
    print("=" * 100)
    print(f"ğŸ“ STEP 1: QUERY DECOMPOSITION")
    print(f"Original query: '{query}'")

    decomp_prompt = DECOMPOSITION_PROMPT.format(query=query)
    decomp_response = call_llm(decomp_prompt, step_name="QUERY DECOMPOSITION", expect_json=True)

    # ç°åœ¨ decomp_response å·²ç»æ˜¯è§£æåçš„å­—å…¸
    if isinstance(decomp_response, dict):
        needs_decomp = decomp_response.get("needs_decomposition", False)
        sub_queries = decomp_response.get("sub_queries", [])
        print(f"âœ… Decomposition result: needs_decomposition={needs_decomp}, sub_queries={sub_queries}")
    else:
        print(f"âŒ Unexpected response type for decomposition: {type(decomp_response)}")
        needs_decomp = False
        sub_queries = []

    queries = sub_queries if needs_decomp else [query]
    print(f"ğŸ“‹ Queries to process: {queries}")

    sub_answers = []

    for i, q in enumerate(queries):
        print("=" * 100)
        print(f"ğŸ”„ PROCESSING SUB-QUERY {i + 1}/{len(queries)}: '{q}'")

        current_query = q
        max_retries = 3
        is_relevant = False

        for attempt in range(max_retries):
            print(f"ğŸ” ATTEMPT {attempt + 1}/{max_retries}")
            print(f"   Current query: '{current_query}'")

            # æ£€ç´¢æ–‡æ¡£
            doc_texts, doc_ids = retrieve_documents(current_query, bm25_retriever, bge_retriever, bge_reranker,
                                                    doc_id_to_text)
            documents_str = "\n".join([f"Doc {i + 1}: {text}" for i, text in enumerate(doc_texts)])

            # æ­¥éª¤ 2: ç›¸å…³æ€§æ£€æŸ¥
            print("=" * 100)
            print(f"ğŸ“Š STEP 2.{attempt + 1}: RELEVANCE CHECK")
            rel_prompt = RELEVANCE_CHECK_PROMPT.format(query=current_query, documents=documents_str)
            rel_response = call_llm(rel_prompt, step_name=f"RELEVANCE CHECK (Attempt {attempt + 1})", expect_json=True)

            # ç°åœ¨ rel_response å·²ç»æ˜¯è§£æåçš„å­—å…¸
            if isinstance(rel_response, dict):
                is_relevant = rel_response.get("is_relevant", False)
                reason = rel_response.get("reason", "")
                suggested_rewrite = rel_response.get("suggested_rewrite", "")
                print(f"   âœ… Relevance check result: is_relevant={is_relevant}, reason={reason}")
                if suggested_rewrite:
                    print(f"   ğŸ’¡ Suggested rewrite: {suggested_rewrite}")
            else:
                print(f"   âŒ Unexpected response type for relevance check: {type(rel_response)}")
                is_relevant = False
                reason = "Unexpected response type"
                suggested_rewrite = ""

            if is_relevant:
                print(f"   âœ… Documents are relevant, proceeding to answer generation")
                break
            else:
                print(f"   âš ï¸ Documents not relevant, attempting query rewrite")
                # é‡å†™æŸ¥è¯¢
                rewrite_prompt = QUERY_REWRITE_PROMPT.format(original_query=current_query, reason=reason,
                                                             suggested_rewrite=suggested_rewrite)
                current_query = call_llm(rewrite_prompt, step_name=f"QUERY REWRITE (Attempt {attempt + 1})")
                print(f"   ğŸ”„ Rewrote query to: '{current_query}'")

        if not is_relevant:
            print(f"   âŒ Failed to find relevant documents after {max_retries} attempts")
            sub_answers.append("Insufficient information after retries.")
            continue

        # æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ
        print("=" * 100)
        print(f"ğŸ“ STEP 3: GENERATE ANSWER")
        context = "\n\n".join(doc_texts)
        gen_prompt = GENERATE_ANSWER_PROMPT.format(query=current_query, context=context)
        gen_response = call_llm(gen_prompt, step_name="GENERATE ANSWER")

        if "\nEvidence: " in gen_response:
            answer, evidence = gen_response.split("\nEvidence: ", 1)
            print(f"âœ… Answer generated with evidence")
            print(f"ğŸ’¡ Answer: {answer}")
            print(f"ğŸ“š Evidence: {evidence[:200]}...")
        else:
            answer = gen_response
            evidence = ""
            print(f"âœ… Answer generated (no evidence separated)")
            print(f"ğŸ’¡ Answer: {answer}")

        # è‡ªæ£€
        print("=" * 100)
        print(f"âœ… STEP 4: SELF-CHECK")
        self_check_prompt = SELF_CHECK_PROMPT.format(answer=answer, documents=documents_str)
        self_check_response = call_llm(self_check_prompt, step_name="SELF-CHECK", expect_json=True)

        # ç°åœ¨ self_check_response å·²ç»æ˜¯è§£æåçš„å­—å…¸
        if isinstance(self_check_response, dict):
            is_valid = self_check_response.get("is_valid", False)
            issues = self_check_response.get("issues", "")
            revised_answer = self_check_response.get("revised_answer", "")
            print(f"ğŸ” Self-check result: is_valid={is_valid}, issues={issues}")
            if revised_answer:
                print(f"ğŸ“ Revised answer: {revised_answer}")
        else:
            print(f"âŒ Unexpected response type for self-check: {type(self_check_response)}")
            is_valid = False
            issues = "Unexpected response type"
            revised_answer = ""

        final_sub_answer = revised_answer if not is_valid else answer
        sub_answers.append(final_sub_answer)
        print(f"âœ… Final sub-answer: {final_sub_answer}")

    # å¦‚æœåˆ†è§£äº†ï¼Œåˆ™åˆæˆç­”æ¡ˆ
    print("=" * 100)
    print(f"ğŸ¯ FINAL STEP: SYNTHESIZE ANSWERS")
    if needs_decomp and len(sub_answers) > 1:
        print(f"ğŸ“¦ Synthesizing {len(sub_answers)} sub-answers into final answer")
        sub_answers_str = "\n".join([f"Sub-answer {i + 1}: {answer}" for i, answer in enumerate(sub_answers)])
        synth_prompt = SYNTHESIZE_ANSWERS_PROMPT.format(original_query=query, sub_answers=sub_answers_str)
        final_answer = call_llm(synth_prompt, step_name="SYNTHESIZE ANSWERS")
        print(f"âœ… Final synthesized answer ready")
    else:
        final_answer = sub_answers[0] if sub_answers else "No answer generated"
        print(f"âœ… Using single answer as final answer")

    return final_answer


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    sample_query = "Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?"
    print("ğŸš€ STARTING RAG PIPELINE DEMO")
    print("=" * 100)
    answer = rag_pipeline(sample_query)
    print(f"ğŸ‰ FINAL RESULT")
    print("=" * 100)
    print(f"ğŸ“ Original Query: {sample_query}")
    print(f"ğŸ’¡ Final Answer: {answer}")
    print("=" * 100)