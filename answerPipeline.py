import json
import requests
import config  # å‡è®¾ config.py å­˜åœ¨ï¼ŒåŒ…å« BASE_DATA_DIR, SF_API_KEY ç­‰é…ç½®
from BGEReranker import BGEReranker  # BGE é‡æ’åºå™¨æ¨¡å—
from BM25Retriever import BM25Retriever  # BM25 æ£€ç´¢å™¨æ¨¡å—
from HQSmallDataLoader import HQSmallDataLoader  # HotpotQA å°å‹æ•°æ®é›†åŠ è½½å™¨
from denseInstructionRetriever import Qwen3Retriever  # Qwen3 å¯†é›†æ£€ç´¢å™¨
from denseRetriever import BGERetriever  # BGE å¯†é›†æ£€ç´¢å™¨
from hybridRetrieveRerank import hybrid_retrieve_and_rerank  # æ··åˆæ£€ç´¢å’Œé‡æ’åºå‡½æ•°
from prompts import DECOMPOSITION_PROMPT, RELEVANCE_CHECK_PROMPT, QUERY_REWRITE_PROMPT, GENERATE_ANSWER_PROMPT, \
    SELF_CHECK_PROMPT, SYNTHESIZE_ANSWERS_PROMPT  # å¯¼å…¥é¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿


# å‡½æ•°ï¼šè°ƒç”¨ SiliconFlow API æ¥ç”Ÿæˆ LLM å“åº”
def call_llm(prompt, max_tokens=512, temperature=0.7, step_name=""):
    """
    è°ƒç”¨ SiliconFlow API æ¥å¤„ç†ç»™å®šçš„æç¤ºã€‚
    å‚æ•°:
    prompt (str): è¾“å…¥çš„æç¤ºæ–‡æœ¬ã€‚
    max_tokens (int): æœ€å¤§ç”Ÿæˆçš„ token æ•°é‡ï¼Œé»˜è®¤ 512ã€‚
    temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.7ã€‚
    step_name (str): å½“å‰æ­¥éª¤åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
    è¿”å›:
    str: API è¿”å›çš„å“åº”å†…å®¹ã€‚
    å¼‚å¸¸:
    å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ã€‚
    """
    print(f"\n{'=' * 80}")
    print(f"ğŸ¤– LLM CALL - {step_name}")
    print(f"{'=' * 80}")
    print(f"ğŸ“¤ PROMPT SENT:\n{prompt}")
    print(f"{'-' * 80}")

    headers = {
        "Authorization": f"Bearer {config.SF_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": config.SF_LLM_MODEL_NAME,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(config.SF_API_LLM_URL, headers=headers, json=data)
    if response.status_code == 200:
        result = response.json()["choices"][0]["message"]["content"].strip()
        print(f"ğŸ“¥ RESPONSE RECEIVED:\n{result}")
        print(f"{'=' * 80}")
        return result
    else:
        error_msg = f"API call failed: {response.text}"
        print(f"âŒ ERROR: {error_msg}")
        print(f"{'=' * 80}")
        raise Exception(error_msg)


# å‡½æ•°ï¼šåˆå§‹åŒ–æ£€ç´¢å™¨
def initialize_retrievers():
    """
    åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’Œå„ç§æ£€ç´¢å™¨ã€‚
    è¿”å›:
    tuple: åŒ…å« BM25 æ£€ç´¢å™¨ã€BGE æ£€ç´¢å™¨ã€Qwen3 æ£€ç´¢å™¨ã€BGE é‡æ’åºå™¨ å’Œ æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    """
    print("ğŸ”„ Initializing retrievers...")
    data_loader = HQSmallDataLoader(config.BASE_DATA_DIR)
    all_doc_ids, all_documents = data_loader.load_collection(config.COLLECTION_PATH)
    doc_id_to_text = dict(zip(all_doc_ids, all_documents))

    if config.SF_API_KEY:
        bge_reranker = BGEReranker(api_key=config.SF_API_KEY)
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever(api_key=config.SF_API_KEY)
        bge_retriever = BGERetriever(api_key=config.SF_API_KEY)
    else:
        bge_reranker = BGEReranker()
        bm25_retriever = BM25Retriever()
        qwen3_retriever = Qwen3Retriever()
        bge_retriever = BGERetriever()

    bm25_retriever.load_index(config.BM25_INDEX_PATH)
    bge_retriever.load_index(config.BGE_INDEX_DIR)
    qwen3_retriever.load_index(config.QWEN_INDEX_DIR)
    print("âœ… Retrievers initialized successfully")
    return bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text


# å‡½æ•°ï¼šæ£€ç´¢å’Œæ ¼å¼åŒ–æ–‡æ¡£
def retrieve_documents(query, bm25_retriever, bge_retriever, bge_reranker, doc_id_to_text, retrieval_top_k=50,
                       rerank_top_k=10):
    """
    ä½¿ç”¨æ··åˆæ£€ç´¢å’Œé‡æ’åºæ¥æ£€ç´¢æ–‡æ¡£ã€‚
    å‚æ•°:
    query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
    bm25_retriever: BM25 æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_retriever: BGE æ£€ç´¢å™¨å®ä¾‹ã€‚
    bge_reranker: BGE é‡æ’åºå™¨å®ä¾‹ã€‚
    doc_id_to_text (dict): æ–‡æ¡£ ID åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚
    retrieval_top_k (int): åˆå§‹æ£€ç´¢çš„ top k å€¼ï¼Œé»˜è®¤ 50ã€‚
    rerank_top_k (int): é‡æ’åºåçš„ top k å€¼ï¼Œé»˜è®¤ 10ã€‚
    è¿”å›:
    tuple: åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨å’Œæ–‡æ¡£ ID åˆ—è¡¨ã€‚
    """
    print(f"\nğŸ” RETRIEVING DOCUMENTS FOR QUERY: '{query}'")
    print(f"Retrieval top_k: {retrieval_top_k}, Rerank top_k: {rerank_top_k}")

    results = hybrid_retrieve_and_rerank(
        query=query,
        first_retriever=bm25_retriever,
        second_retriever=bge_retriever,
        reranker=bge_reranker,
        doc_id_to_text_map=doc_id_to_text,
        retrieval_top_k=retrieval_top_k,
        rerank_top_k=rerank_top_k
    )
    doc_texts = [doc_id_to_text[doc_id] for doc_id, _ in results]
    doc_ids = [doc_id for doc_id, _ in results]

    print(f"âœ… Retrieved {len(doc_texts)} documents")
    for i, (doc_id, text) in enumerate(zip(doc_ids, doc_texts)):
        print(f"   Document {i + 1} (ID: {doc_id}): {text[:100]}...")

    return doc_texts, doc_ids


# ç®¡é“å‡½æ•°ï¼šRAG ç®¡é“å®ç°
def rag_pipeline(query):
    """
    RAG (Retrieval-Augmented Generation) ç®¡é“çš„ä¸»å‡½æ•°ã€‚
    å¤„ç†æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åˆ†è§£ã€æ£€ç´¢ã€ç›¸å…³æ€§æ£€æŸ¥ã€é‡å†™ã€ç”Ÿæˆç­”æ¡ˆã€è‡ªæ£€å’Œåˆæˆã€‚
    å‚æ•°:
    query (str): è¾“å…¥æŸ¥è¯¢ã€‚
    è¿”å›:
    str: æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆã€‚
    """
    print(f"\nğŸ¯ STARTING RAG PIPELINE FOR QUERY: '{query}'")
    print("=" * 100)

    bm25_retriever, bge_retriever, qwen3_retriever, bge_reranker, doc_id_to_text = initialize_retrievers()

    # æ­¥éª¤ 1: æŸ¥è¯¢åˆ†è§£
    print(f"\nğŸ“ STEP 1: QUERY DECOMPOSITION")
    print(f"Original query: '{query}'")

    decomp_prompt = DECOMPOSITION_PROMPT.format(query=query)
    decomp_response = call_llm(decomp_prompt, step_name="QUERY DECOMPOSITION")

    try:
        decomp_json = json.loads(decomp_response)
        needs_decomp = decomp_json["needs_decomposition"]
        sub_queries = decomp_json["sub_queries"]
        print(f"âœ… Decomposition result: needs_decomposition={needs_decomp}, sub_queries={sub_queries}")
    except Exception as e:
        print(f"âŒ Failed to parse decomposition response: {e}")
        needs_decomp = False
        sub_queries = []

    queries = sub_queries if needs_decomp else [query]
    print(f"ğŸ“‹ Queries to process: {queries}")

    sub_answers = []

    for i, q in enumerate(queries):
        print(f"\nğŸ”„ PROCESSING SUB-QUERY {i + 1}/{len(queries)}: '{q}'")

        current_query = q
        max_retries = 3
        is_relevant = False

        for attempt in range(max_retries):
            print(f"\n   ğŸ” ATTEMPT {attempt + 1}/{max_retries}")
            print(f"   Current query: '{current_query}'")

            # æ£€ç´¢æ–‡æ¡£
            doc_texts, doc_ids = retrieve_documents(current_query, bm25_retriever, bge_retriever, bge_reranker,
                                                    doc_id_to_text)
            documents_str = "\n".join([f"Doc {i + 1}: {text}" for i, text in enumerate(doc_texts)])

            # æ­¥éª¤ 2: ç›¸å…³æ€§æ£€æŸ¥
            print(f"\n   ğŸ“Š STEP 2.{attempt + 1}: RELEVANCE CHECK")
            rel_prompt = RELEVANCE_CHECK_PROMPT.format(query=current_query, documents=documents_str)
            rel_response = call_llm(rel_prompt, step_name=f"RELEVANCE CHECK (Attempt {attempt + 1})")

            try:
                rel_json = json.loads(rel_response)
                is_relevant = rel_json["is_relevant"]
                reason = rel_json["reason"]
                suggested_rewrite = rel_json["suggested_rewrite"]
                print(f"   âœ… Relevance check result: is_relevant={is_relevant}, reason={reason}")
                if suggested_rewrite:
                    print(f"   ğŸ’¡ Suggested rewrite: {suggested_rewrite}")
            except Exception as e:
                print(f"   âŒ Failed to parse relevance check response: {e}")
                is_relevant = False
                reason = "Parsing error"
                suggested_rewrite = ""

            if is_relevant:
                print(f"   âœ… Documents are relevant, proceeding to answer generation")
                break
            else:
                print(f"   âš ï¸ Documents not relevant, attempting query rewrite")
                # é‡å†™æŸ¥è¯¢
                rewrite_prompt = QUERY_REWRITE_PROMPT.format(original_query=current_query, reason=reason,
                                                             suggested_rewrite=suggested_rewrite)
                current_query = call_llm(rewrite_prompt, step_name=f"QUERY REWRITE (Attempt {attempt + 1})")
                print(f"   ğŸ”„ Rewrote query to: '{current_query}'")

        if not is_relevant:
            print(f"   âŒ Failed to find relevant documents after {max_retries} attempts")
            sub_answers.append("Insufficient information after retries.")
            continue

        # æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ
        print(f"\n   ğŸ“ STEP 3: GENERATE ANSWER")
        context = "\n\n".join(doc_texts)
        gen_prompt = GENERATE_ANSWER_PROMPT.format(query=current_query, context=context)
        gen_response = call_llm(gen_prompt, step_name="GENERATE ANSWER")

        if "\nEvidence: " in gen_response:
            answer, evidence = gen_response.split("\nEvidence: ", 1)
            print(f"   âœ… Answer generated with evidence")
            print(f"   ğŸ’¡ Answer: {answer}")
            print(f"   ğŸ“š Evidence: {evidence[:200]}...")
        else:
            answer = gen_response
            evidence = ""
            print(f"   âœ… Answer generated (no evidence separated)")
            print(f"   ğŸ’¡ Answer: {answer}")

        # è‡ªæ£€
        print(f"\n   âœ… STEP 4: SELF-CHECK")
        self_check_prompt = SELF_CHECK_PROMPT.format(answer=answer, documents=documents_str)
        self_check_response = call_llm(self_check_prompt, step_name="SELF-CHECK")

        try:
            self_check_json = json.loads(self_check_response)
            is_valid = self_check_json["is_valid"]
            issues = self_check_json["issues"]
            revised_answer = self_check_json["revised_answer"]
            print(f"   ğŸ” Self-check result: is_valid={is_valid}, issues={issues}")
            if revised_answer:
                print(f"   ğŸ“ Revised answer: {revised_answer}")
        except Exception as e:
            print(f"   âŒ Failed to parse self-check response: {e}")
            is_valid = False
            issues = "Parsing error"
            revised_answer = ""

        final_sub_answer = revised_answer if not is_valid else answer
        sub_answers.append(final_sub_answer)
        print(f"   âœ… Final sub-answer: {final_sub_answer}")

    # å¦‚æœåˆ†è§£äº†ï¼Œåˆ™åˆæˆç­”æ¡ˆ
    print(f"\nğŸ¯ FINAL STEP: SYNTHESIZE ANSWERS")
    if needs_decomp and len(sub_answers) > 1:
        print(f"ğŸ“¦ Synthesizing {len(sub_answers)} sub-answers into final answer")
        sub_answers_str = "\n".join([f"Sub-answer {i + 1}: {answer}" for i, answer in enumerate(sub_answers)])
        synth_prompt = SYNTHESIZE_ANSWERS_PROMPT.format(original_query=query, sub_answers=sub_answers_str)
        final_answer = call_llm(synth_prompt, step_name="SYNTHESIZE ANSWERS")
        print(f"âœ… Final synthesized answer ready")
    else:
        final_answer = sub_answers[0] if sub_answers else "No answer generated"
        print(f"âœ… Using single answer as final answer")

    return final_answer


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    sample_query = "Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?"
    print("ğŸš€ STARTING RAG PIPELINE DEMO")
    print("=" * 100)
    answer = rag_pipeline(sample_query)
    print(f"\nğŸ‰ FINAL RESULT")
    print("=" * 100)
    print(f"ğŸ“ Original Query: {sample_query}")
    print(f"ğŸ’¡ Final Answer: {answer}")
    print("=" * 100)